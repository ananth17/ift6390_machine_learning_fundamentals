\documentclass{article} % For LaTeX2e
\usepackage{nips14submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage[numbers,sort&compress]{natbib}
 \usepackage{graphicx}
 \usepackage[export]{adjustbox}
\DeclareGraphicsExtensions{.pdf,.png,.jpg,.eps}

%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09         


\title{Messing with linear classifiers}


\author{
Gabriel C-Parent\\
Département d’informatique et recherche opérationnelle\\
Université de Montréal\\
\texttt{gabriel.c-parent@umontreal.ca} \\
\And
Dora Fugère \\
Département de mathématiques et de statistique \\
Université de Montréal\\
\texttt{dora.fugere@umontreal.ca} \\
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}

\maketitle

\begin{abstract}
% still needs much work
Adversarial examples generation from input space in neural network
has shown that these powerful constructs can be manipulated into misclassifying
previously well classified examples by adding an imperceptible amount of
distortion. Using this methodology, we investigate the relative robustness of
simple classifiers.
\end{abstract}


\section{Introduction}

% new problems vs progress
Recently, neural networks have been brought under questionning. The smoothness
assumption, the idea that imperceptible distortion of input shouldn't change
the output was shown not to hold \citep{szegedy_intriguing_2013}. This is a
remarkable finding since smoothness was assumed to be a necessary property of the
learning process.
This comes in stark constrast with feats such as automatic image description
\citep{vinyals_show_2014} and large-scale multi-character text recognition
\citep{goodfellow_multi-digit_2013} to name but a few.

% comprehensibility is important
As for most real-world problems, there are many desirable and often conflicting
goals when using machine learning. Amongst them speed, accuracy and simplicity
are easy to justify. We'll focus on comprehensibility, because that justifies
us using a simpler model.
We interpret simplicity as ``given two models with the same generalization error, the more
comprehensible one should be preferred" \citep{domingos_role_1999}.
This obviously is dependent on multiple other factors (e.g. speed and accuracy)
but it does sound like the \textit{keep it simple stupid} rule of thumb.
Furthermore, as stated in \citep{hand_classifier_2006}, empirical comparison of
performance is very context-dependent and can be infludenced by treatments such 
as the preprocessing steps, training parameters and model hyperparameters.

% experiments executed
Inspired by the methodology to induce misclassification, we wondered
if a similar optimization procedure could be applied to generate adversarial
examples in a simpler classifier. For this purpose, we used a linear support
vector machine (SVM). The only other contender would have been Naive Bayes,
but we happen to like sklearn's implementation of the linear SVM \footnote{we wouldn't
risk reinventing the square wheel} \citep{pedregosa_scikit-learn:_2011}.
We report the robustness of our optimization procedure, the results on classifiers
trained with different loss and regularization parameters and we then feed the 
generated adversarial examples to a neural network to see if some underlying
feature of the image was captured.


\section{Framework}


\subsection{Dataset}

The experiments were performed on the MNIST dataset \citep{lecun_mnist_1998}.

Let \begin{math} X=\{0, 255\}^{784} \end{math}, the input domain.
This is the set of $28\times28$ 8-bit image.

Let \begin{math} Y = \{0, 9\} \end{math}, the output domain.
This is the set of valid classes for an MNIST digit.


\subsection{Optimization goal}

Let \begin{math} f:X \rightarrow  Y \end{math} a classifier mapping $x_i \in
X$ to $y_i \in Y$.

We aim to solve the following optimization

\begin{equation}
\label{eq:optimization}
\begin{aligned}
& {\text{minimize}}
&   \lVert{r} \rVert^2\\
& \text{subject to} \\
& & x_i + r \in X \\
& & f(x_i) \neq f(x_i + r)
\end{aligned}
\end{equation}

This is quite similar to \citep{szegedy_intriguing_2013} but the newly
generated images remain 8-bit to stay in the input domain of the MNIST dataset.
Sadly, this also makes it a discrete optimization problem.


\subsection{Optimization goal for the linear SVM}

Suppose we want to misclassy an arbitrary image $x_i$ correctly classified
as $y_1$ by adding a vector $r$ of distortion in a two-class setting.

The classifier classifies the input based on the following decision function.

\begin{equation}
\label{eq:decision_function}
  y_i = argmax (x_i \cdot W^T + b)
\end{equation}


The difference between the class weights of the classifier is $W_{diff}$.

\begin{equation}
\label{eq:difference}
  W_{diff} = W_2 - W_1
\end{equation}

The distance between the values of the two classes is $d$.
\begin{equation}
\label{eq:gap}
  d = x_i \cdot W_{diff}^T
\end{equation}


To cause misclassification, $r$ must respect the following constraint:
\begin{equation}
\label{eq:noise_threshold}
  r \cdot W_{diff}^T > d
\end{equation}

What we need is to find the smallest $ \lVert{r} \rVert^2$ that will cause
misclassification. Note that when there are more than two classes, we just
apply the procedure to all other classes $y_i \neq y_1$ and choose the one
with minimal squared euclidean norm.


\subsection{Knapsack problem and the greedy approach}

The problem is similar to the bounded multiple-class binary Knapsack problem
\citep{vanderbeck_extending_2002}, with the difference that we are searching
for the smallest knapsack holding a value superior to $d$ (equation
\ref{eq:gap}).

The exact algorithm would be too costly for our purpose so we chose to use a
greedy heuristic inspired by Dantzig's \citep{dantzig_discrete-variable_1957}.





\section{Experimental results}


\subsection{Precision of optimization procedure}

Although the bounds on the optimization procedure could be arbitrary big, it
is usually small for this problem, because $d$ is usually quite big relative
to the size of weight increments. This means tight bounds on the possible
true value of the squared euclidean norm.

%\begin{figure}[h!]
%\includegraphics[scale=0.5]{experiment1}
%\caption{\small Experiment 1}
%\end{figure}


\subsection{Regularization schemes}

We wanted to observe the effects of different regularization methods and
parameters on the smallest squared euclidean norm possible to the ge

\begin{figure}[h!]
\includegraphics[scale=0.5]{experiment2_l2}
\caption{\small Improvement of reliability with regularization}
\end{figure}

\subsection{}

\section{Discussion}

As expected, having bigger weights allows the generation of adversarial examples with less
squared euclidean norm.


\subsubsection*{Acknowledgments}
We would like to thank caffein.


\bibliographystyle{acm}


\bibliography{project}


%%%%%%%%%% supplemental materials %%%%%%%%%%
\newpage



%\begin{center}
%\textbf{\large Supplemental Materials}
%\end{center}

\subsection{example of the greedy heuristic}

We encode the cost of adding a bit of distortion to a pixel in the form of a
volume. For example, given vector of distortion $r$ = [0, 3], the costs of
adding 1 unit of distortion for each position of $r$ is [1, 7]. For the values,
we use the absolute value of vector $W_{diff}$. If $W_{diff}$ = [-5, 10], the 
$\frac {value} {volume}$ ratios would be [5, $\frac{10}{7}$]. The best choice
to improve the value 

We iteratively choose the elements with the best until
the critical value is $d$ is passed.

At iteration $n$, the choices made are optimal of the knapsack of size
$K_n = \sum\limits_{i=1}^n w_i$. This is the case because every object in the
knapsack was chosen with maximum $\frac{value} {volume}$ ratio and the knapsack
is filled perfectly.

Let $(K_1, v_1)$ and $(K_2, v_2)$, the size and value of knapsacks at two
iterations, we know that if $K_1 < K_2$ then $v_1 \leq v_2$, that is to say
it's a monotonically increasing function. The inverse is also true, if
$v_1 < v_2$ then $K_1 < K_2$.

For our purpose, this means that the true value of the norm
$\lVert{r} \rVert^2 \in [K_{n-1}, K_n]$ if the algorithm finishes at iteration
$n$. This boundary can be very small or very big, depending on the size of the
weight increments. We measured the boundary size to show that in this problem,
we have very little incertitude.


\subsection{regularization methods and minimal squared euclidean norm}

\begin{figure}[h!]
\includegraphics[scale=0.5]{experiment2_l1}
\caption{\small Improvement of reliability with regularization}
\end{figure}


\begin{figure}[h!]
\includegraphics[scale=0.5]{experiment2_el}
\caption{\small Improvement of reliability with regularization}
\end{figure}



\end{document}


